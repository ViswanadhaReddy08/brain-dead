{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z9Dzb93ALAkk",
        "outputId": "54036c99-6d65-4946-a1ce-198837b90f33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Path to the dataset in Google Drive\n",
        "dataset_path = '/content/drive/MyDrive/brain dead/Brain Dead CompScholar Dataset.csv'\n",
        "\n",
        "# Load the dataset\n",
        "import pandas as pd\n",
        "df = pd.read_csv(dataset_path)\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCMVrKh2Nycb",
        "outputId": "46dda1cf-9c23-4d7b-855a-f879102db375"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Paper Id                                        Paper Title  \\\n",
            "0         1  Multi-document Summarization via Deep Learning...   \n",
            "1         2  NLP based Machine Learning Approaches for Text...   \n",
            "2         3  Abstractive text summarization using LSTM-CNN ...   \n",
            "3         4  DEXPERTS: Decoding-Time Controlled Text Genera...   \n",
            "4         5     A Survey of Knowledge-enhanced Text Generation   \n",
            "\n",
            "                                           Key Words  \\\n",
            "0  Multi-document summarization (MDS), Deep learn...   \n",
            "1  Text summarization, Abstractive and extractive...   \n",
            "2  Text mining . Abstractive text summarization ....   \n",
            "3  Natural language generation, Controlled text g...   \n",
            "4  text-to-text generation, natural language proc...   \n",
            "\n",
            "                                            Abstract  \\\n",
            "0  Multi-document summarization (MDS) is an effec...   \n",
            "1  Due to the plethora of data available today, t...   \n",
            "2   Abstractive Text Summarization (ATS), which i...   \n",
            "3  Despite recent advances in natural language\\ng...   \n",
            "4  The goal of text-to-text generation is to make...   \n",
            "\n",
            "                                          Conclusion  \\\n",
            "0  In this article, we have presented the first c...   \n",
            "1  We have seen that due to abundant availability...   \n",
            "2  In this paper, we develop a novel LSTM-CNN bas...   \n",
            "3  We present DEXPERTS, a method for controlled\\n...   \n",
            "4  In this survey, we present a comprehensive rev...   \n",
            "\n",
            "                                            Document  \\\n",
            "0  Multi-document Summarization via Deep Learning...   \n",
            "1  NLP based Machine Learning Approaches for Text...   \n",
            "2  Abstractive text summarization using LSTM-CNN ...   \n",
            "3  DEXPERTS: Decoding-Time Controlled Text Genera...   \n",
            "4  A Survey of Knowledge-enhanced Text Generation...   \n",
            "\n",
            "                    Paper Type  \\\n",
            "0           Text summarization   \n",
            "1  Natural Language Processing   \n",
            "2           Text summarization   \n",
            "3              Text generation   \n",
            "4              Text generation   \n",
            "\n",
            "                                             Summary  \\\n",
            "0  This article presents a systematic overview of...   \n",
            "1  The article discusses the importance of text s...   \n",
            "2  The article presents a new framework for abstr...   \n",
            "3  The paper proposes a method called DEXPERTS fo...   \n",
            "4  The paper discusses the challenges in text-to-...   \n",
            "\n",
            "                         Topic  \\\n",
            "0  Natural Language Processing   \n",
            "1  Natural Language Processing   \n",
            "2  Natural Language Processing   \n",
            "3  Natural Language Processing   \n",
            "4  Natural Language Processing   \n",
            "\n",
            "                                                 OCR  \\\n",
            "0  lla i aye RR | poe [Sena Sena | Sena, ‚Äî+ ar ea...   \n",
            "1  @STOM ¬© Word Vector Embedding kenearest neighb...   \n",
            "2  encoder decoderWord Merpholosical Coreterence ...   \n",
            "3  reatva star on negative proms oe TT os ee oe S...   \n",
            "4  (ira => Generation model => foam] | Generation...   \n",
            "\n",
            "                               labels  \n",
            "0  Deep Learning and Machine Learning  \n",
            "1  Deep Learning and Machine Learning  \n",
            "2  Deep Learning and Machine Learning  \n",
            "3  Deep Learning and Machine Learning  \n",
            "4  Deep Learning and Machine Learning  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "import torch\n",
        "import time\n",
        "import evaluate\n",
        "from transformers import (\n",
        "    T5Tokenizer, T5ForConditionalGeneration,\n",
        "    BartTokenizer, BartForConditionalGeneration,\n",
        "    PegasusTokenizer, PegasusForConditionalGeneration\n",
        ")\n",
        "\n",
        "# üöÄ Load Evaluation Metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "# üìå Load Models and Tokenizers\n",
        "models = {\n",
        "    \"DistilBART\": {\n",
        "        \"model\": BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\"),\n",
        "        \"tokenizer\": BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\"),\n",
        "    },\n",
        "    \"T5-Small\": {\n",
        "        \"model\": T5ForConditionalGeneration.from_pretrained(\"t5-small\"),\n",
        "        \"tokenizer\": T5Tokenizer.from_pretrained(\"t5-small\"),\n",
        "    },\n",
        "    \"PEGASUS-XSum\": {\n",
        "        \"model\": PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-xsum\"),\n",
        "        \"tokenizer\": PegasusTokenizer.from_pretrained(\"google/pegasus-xsum\"),\n",
        "    }\n",
        "}\n",
        "\n",
        "# üìù Example Texts for Summarization\n",
        "texts = [\n",
        "    \"The rapid advancement of artificial intelligence in scientific research has led to an increase in automated summarization models...\",\n",
        "    \"Deep learning models have revolutionized text summarization by introducing transformer-based architectures...\"\n",
        "]\n",
        "reference_summaries = [\n",
        "    \"AI-driven models enhance scientific text summarization.\",\n",
        "    \"Transformers have improved automated text summarization.\"\n",
        "]\n",
        "\n",
        "num_samples = len(texts)  # Ensure slicing does not cause an IndexError\n",
        "\n",
        "# ‚ö° Function for Model Evaluation\n",
        "def evaluate_model(model_name, model, tokenizer, texts, reference_summaries):\n",
        "    total_time = 0\n",
        "    generated_summaries = []\n",
        "\n",
        "    for text in texts:\n",
        "        inputs = tokenizer(text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
        "        start_time = time.time()\n",
        "        summary_ids = model.generate(**inputs, max_length=120, min_length=40)\n",
        "        elapsed_time = time.time() - start_time\n",
        "        total_time += elapsed_time\n",
        "\n",
        "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "        generated_summaries.append(summary)\n",
        "\n",
        "    rouge_scores = rouge.compute(predictions=generated_summaries, references=reference_summaries)\n",
        "    bleu_score = bleu.compute(predictions=generated_summaries, references=reference_summaries)[\"bleu\"]\n",
        "\n",
        "    avg_time = total_time / len(texts)\n",
        "    memory_usage = torch.cuda.memory_allocated() / (1024 ** 2) if torch.cuda.is_available() else \"N/A\"\n",
        "\n",
        "    return {\n",
        "         \"Model\": model_name,\n",
        "        \"ROUGE-1\": rouge_scores[\"rouge1\"] * 100,  # Access fmeasure directly\n",
        "        \"ROUGE-2\": rouge_scores[\"rouge2\"] * 100,  # Access fmeasure directly\n",
        "        \"ROUGE-L\": rouge_scores[\"rougeL\"] * 100,  # Access fmeasure directly\n",
        "        \"BLEU\": bleu_score * 100,\n",
        "        \"Inference Speed (s)\": round(avg_time, 3),\n",
        "        \"Memory Usage (MB)\": memory_usage\n",
        "    }\n",
        "\n",
        "# üèÜ Run Evaluation for Each Model\n",
        "results = []\n",
        "for model_name, model_data in models.items():\n",
        "    result = evaluate_model(model_name, model_data[\"model\"], model_data[\"tokenizer\"], texts, reference_summaries)\n",
        "    results.append(result)\n",
        "\n",
        "# üìä Print Evaluation Results\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AlB5ZHXpN07-",
        "outputId": "bff4875e-2295-46e9-b91a-334e7e0d29a7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.12.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.14)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.18.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-xsum and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "          Model    ROUGE-1   ROUGE-2    ROUGE-L  BLEU  Inference Speed (s)  \\\n",
            "0    DistilBART  14.459930  2.500000  12.020906   0.0               10.228   \n",
            "1      T5-Small  26.442308  7.142857  26.442308   0.0                2.079   \n",
            "2  PEGASUS-XSum   7.692308  0.000000   7.692308   0.0               20.446   \n",
            "\n",
            "  Memory Usage (MB)  \n",
            "0               N/A  \n",
            "1               N/A  \n",
            "2               N/A  \n"
          ]
        }
      ]
    }
  ]
}